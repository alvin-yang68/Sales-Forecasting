#
train$review = gsub('<.*?>', ' ', train$review)
it_train = itoken(train$review,
preprocessor = tolower,
tokenizer = word_tokenizer)
vectorizer = vocab_vectorizer(create_vocabulary(myvocab,
ngram = c(1L, 2L)))
dtm_train = create_dtm(it_train, vectorizer)
mylogit.cv = cv.glmnet(x = dtm_train,
y = train$sentiment,
alpha = 0,
family='binomial',
type.measure = "auc")
mylogit.fit = glmnet(x = dtm_train,
y = train$sentiment,
alpha = 0,
lambda = mylogit.cv$lambda.min,
family='binomial')
#####################################
test <- read.table("test.tsv", stringsAsFactors = FALSE,
header = TRUE)
#####################################
# Compute prediction
# Store your prediction for test data in a data frame
# "output": col 1 is test$id
#           col 2 is the predited probabilities
test$review <- gsub('<.*?>', ' ', test$review)
it_test = itoken(test$review,
preprocessor = tolower,
tokenizer = word_tokenizer)
dtm_test = create_dtm(it_test, vectorizer)
mypred = predict(mylogit.fit, dtm_test, type = "response")
output = data.frame(id = test$id, prob = as.vector(mypred))
#####################################
write.table(output, file = "mysubmission.txt",
row.names = FALSE, sep='\t')
setwd(paste("split_", j, sep=""))
# move test_y.tsv to this directory
test.y <- read.table("test_y.tsv", header = TRUE)
pred <- read.table("mysubmission.txt", header = TRUE)
pred <- merge(pred, test.y, by="id")
roc_obj <- roc(pred$sentiment, pred$prob)
pROC::auc(roc_obj)
knitr::opts_chunk$set(echo = TRUE)
library(text2vec)
library(slam)
library(glmnet)
library(pROC)
set.seed(5356)
#####################################
# Load your vocabulary and training data
#####################################
myvocab <- scan(file = "myvocab.txt", what = character())
# Remove this
j = 3
setwd(paste("split_", j, sep=""))
train <- read.table("train.tsv", stringsAsFactors = FALSE,
header = TRUE)
#####################################
#
# Train a binary classification model
#
train$review = gsub('<.*?>', ' ', train$review)
it_train = itoken(train$review,
preprocessor = tolower,
tokenizer = word_tokenizer)
vectorizer = vocab_vectorizer(create_vocabulary(myvocab,
ngram = c(1L, 2L)))
dtm_train = create_dtm(it_train, vectorizer)
mylogit.cv = cv.glmnet(x = dtm_train,
y = train$sentiment,
alpha = 0,
family='binomial',
type.measure = "auc")
mylogit.fit = glmnet(x = dtm_train,
y = train$sentiment,
alpha = 0,
lambda = mylogit.cv$lambda.min,
family='binomial')
#####################################
test <- read.table("test.tsv", stringsAsFactors = FALSE,
header = TRUE)
#####################################
# Compute prediction
# Store your prediction for test data in a data frame
# "output": col 1 is test$id
#           col 2 is the predited probabilities
test$review <- gsub('<.*?>', ' ', test$review)
it_test = itoken(test$review,
preprocessor = tolower,
tokenizer = word_tokenizer)
dtm_test = create_dtm(it_test, vectorizer)
mypred = predict(mylogit.fit, dtm_test, type = "response")
output = data.frame(id = test$id, prob = as.vector(mypred))
#####################################
write.table(output, file = "mysubmission.txt",
row.names = FALSE, sep='\t')
setwd(paste("split_", j, sep=""))
# move test_y.tsv to this directory
test.y <- read.table("test_y.tsv", header = TRUE)
pred <- read.table("mysubmission.txt", header = TRUE)
pred <- merge(pred, test.y, by="id")
roc_obj <- roc(pred$sentiment, pred$prob)
pROC::auc(roc_obj)
library(text2vec)
library(slam)
library(glmnet)
library(pROC)
set.seed(5356)
#####################################
# Load your vocabulary and training data
#####################################
myvocab <- scan(file = "myvocab.txt", what = character())
# Remove this
j = 4
setwd(paste("split_", j, sep=""))
train <- read.table("train.tsv", stringsAsFactors = FALSE,
header = TRUE)
#####################################
#
# Train a binary classification model
#
train$review = gsub('<.*?>', ' ', train$review)
it_train = itoken(train$review,
preprocessor = tolower,
tokenizer = word_tokenizer)
vectorizer = vocab_vectorizer(create_vocabulary(myvocab,
ngram = c(1L, 2L)))
dtm_train = create_dtm(it_train, vectorizer)
mylogit.cv = cv.glmnet(x = dtm_train,
y = train$sentiment,
alpha = 0,
family='binomial',
type.measure = "auc")
mylogit.fit = glmnet(x = dtm_train,
y = train$sentiment,
alpha = 0,
lambda = mylogit.cv$lambda.min,
family='binomial')
#####################################
test <- read.table("test.tsv", stringsAsFactors = FALSE,
header = TRUE)
#####################################
# Compute prediction
# Store your prediction for test data in a data frame
# "output": col 1 is test$id
#           col 2 is the predited probabilities
test$review <- gsub('<.*?>', ' ', test$review)
it_test = itoken(test$review,
preprocessor = tolower,
tokenizer = word_tokenizer)
dtm_test = create_dtm(it_test, vectorizer)
mypred = predict(mylogit.fit, dtm_test, type = "response")
output = data.frame(id = test$id, prob = as.vector(mypred))
#####################################
write.table(output, file = "mysubmission.txt",
row.names = FALSE, sep='\t')
setwd(paste("split_", j, sep=""))
# move test_y.tsv to this directory
test.y <- read.table("test_y.tsv", header = TRUE)
pred <- read.table("mysubmission.txt", header = TRUE)
pred <- merge(pred, test.y, by="id")
roc_obj <- roc(pred$sentiment, pred$prob)
pROC::auc(roc_obj)
library(text2vec)
library(slam)
library(glmnet)
library(pROC)
set.seed(5356)
#####################################
# Load your vocabulary and training data
#####################################
myvocab <- scan(file = "myvocab.txt", what = character())
# Remove this
j = 5
setwd(paste("split_", j, sep=""))
train <- read.table("train.tsv", stringsAsFactors = FALSE,
header = TRUE)
#####################################
#
# Train a binary classification model
#
train$review = gsub('<.*?>', ' ', train$review)
it_train = itoken(train$review,
preprocessor = tolower,
tokenizer = word_tokenizer)
vectorizer = vocab_vectorizer(create_vocabulary(myvocab,
ngram = c(1L, 2L)))
dtm_train = create_dtm(it_train, vectorizer)
mylogit.cv = cv.glmnet(x = dtm_train,
y = train$sentiment,
alpha = 0,
family='binomial',
type.measure = "auc")
mylogit.fit = glmnet(x = dtm_train,
y = train$sentiment,
alpha = 0,
lambda = mylogit.cv$lambda.min,
family='binomial')
#####################################
test <- read.table("test.tsv", stringsAsFactors = FALSE,
header = TRUE)
#####################################
# Compute prediction
# Store your prediction for test data in a data frame
# "output": col 1 is test$id
#           col 2 is the predited probabilities
test$review <- gsub('<.*?>', ' ', test$review)
it_test = itoken(test$review,
preprocessor = tolower,
tokenizer = word_tokenizer)
dtm_test = create_dtm(it_test, vectorizer)
mypred = predict(mylogit.fit, dtm_test, type = "response")
output = data.frame(id = test$id, prob = as.vector(mypred))
#####################################
write.table(output, file = "mysubmission.txt",
row.names = FALSE, sep='\t')
setwd(paste("split_", j, sep=""))
# move test_y.tsv to this directory
test.y <- read.table("test_y.tsv", header = TRUE)
pred <- read.table("mysubmission.txt", header = TRUE)
pred <- merge(pred, test.y, by="id")
roc_obj <- roc(pred$sentiment, pred$prob)
pROC::auc(roc_obj)
library(text2vec)
library(slam)
library(glmnet)
library(pROC)
set.seed(5356)
#####################################
# Load your vocabulary and training data
#####################################
myvocab <- scan(file = "myvocab.txt", what = character())
# Remove this
j = 2
setwd(paste("split_", j, sep=""))
train <- read.table("train.tsv", stringsAsFactors = FALSE,
header = TRUE)
#####################################
#
# Train a binary classification model
#
train$review = gsub('<.*?>', ' ', train$review)
it_train = itoken(train$review,
preprocessor = tolower,
tokenizer = word_tokenizer)
vectorizer = vocab_vectorizer(create_vocabulary(myvocab,
ngram = c(1L, 2L)))
dtm_train = create_dtm(it_train, vectorizer)
mylogit.cv = cv.glmnet(x = dtm_train,
y = train$sentiment,
alpha = 0,
family='binomial',
type.measure = "auc")
mylogit.fit = glmnet(x = dtm_train,
y = train$sentiment,
alpha = 0,
lambda = mylogit.cv$lambda.min,
family='binomial')
#####################################
test <- read.table("test.tsv", stringsAsFactors = FALSE,
header = TRUE)
#####################################
# Compute prediction
# Store your prediction for test data in a data frame
# "output": col 1 is test$id
#           col 2 is the predited probabilities
test$review <- gsub('<.*?>', ' ', test$review)
it_test = itoken(test$review,
preprocessor = tolower,
tokenizer = word_tokenizer)
dtm_test = create_dtm(it_test, vectorizer)
mypred = predict(mylogit.fit, dtm_test, type = "response")
output = data.frame(id = test$id, prob = as.vector(mypred))
#####################################
write.table(output, file = "mysubmission.txt",
row.names = FALSE, sep='\t')
setwd(paste("split_", j, sep=""))
# move test_y.tsv to this directory
test.y <- read.table("test_y.tsv", header = TRUE)
pred <- read.table("mysubmission.txt", header = TRUE)
pred <- merge(pred, test.y, by="id")
roc_obj <- roc(pred$sentiment, pred$prob)
pROC::auc(roc_obj)
library(text2vec)
library(slam)
library(glmnet)
library(pROC)
set.seed(5356)
#####################################
# Load your vocabulary and training data
#####################################
myvocab <- scan(file = "myvocab.txt", what = character())
# Remove this
j = 1
setwd(paste("split_", j, sep=""))
train <- read.table("train.tsv", stringsAsFactors = FALSE,
header = TRUE)
#####################################
#
# Train a binary classification model
#
train$review = gsub('<.*?>', ' ', train$review)
it_train = itoken(train$review,
preprocessor = tolower,
tokenizer = word_tokenizer)
vectorizer = vocab_vectorizer(create_vocabulary(myvocab,
ngram = c(1L, 2L)))
dtm_train = create_dtm(it_train, vectorizer)
mylogit.cv = cv.glmnet(x = dtm_train,
y = train$sentiment,
alpha = 0,
family='binomial',
type.measure = "auc")
mylogit.fit = glmnet(x = dtm_train,
y = train$sentiment,
alpha = 0,
lambda = mylogit.cv$lambda.min,
family='binomial')
#####################################
test <- read.table("test.tsv", stringsAsFactors = FALSE,
header = TRUE)
#####################################
# Compute prediction
# Store your prediction for test data in a data frame
# "output": col 1 is test$id
#           col 2 is the predited probabilities
test$review <- gsub('<.*?>', ' ', test$review)
it_test = itoken(test$review,
preprocessor = tolower,
tokenizer = word_tokenizer)
dtm_test = create_dtm(it_test, vectorizer)
mypred = predict(mylogit.fit, dtm_test, type = "response")
output = data.frame(id = test$id, prob = as.vector(mypred))
#####################################
write.table(output, file = "mysubmission.txt",
row.names = FALSE, sep='\t')
setwd(paste("split_", j, sep=""))
# move test_y.tsv to this directory
test.y <- read.table("test_y.tsv", header = TRUE)
pred <- read.table("mysubmission.txt", header = TRUE)
pred <- merge(pred, test.y, by="id")
roc_obj <- roc(pred$sentiment, pred$prob)
pROC::auc(roc_obj)
dim(train)
dim(test)
?gsub
?itoken
knitr::opts_chunk$set(echo = TRUE)
library(text2vec)
library(slam)
library(glmnet)
library(pROC)
set.seed(5356)
?itoken
?create_vocabulary
colnames(dtm_train)
library(glmnet)
library(text2vec)
set.seed(5356)
#####################################
# Load your vocabulary and training data
#####################################
myvocab <- scan(file = "myvocab.txt", what = character())
# Remove this
j = 1
setwd(paste("split_", j, sep=""))
train <- read.table("train.tsv", stringsAsFactors = FALSE,
header = TRUE)
#####################################
#
# Train a binary classification model
#
train$review = gsub('<.*?>', ' ', train$review)
it_train = itoken(train$review,
preprocessor = tolower,
tokenizer = word_tokenizer)
vectorizer = vocab_vectorizer(create_vocabulary(myvocab,
ngram = c(1L, 4L)))
dtm_train = create_dtm(it_train, vectorizer)
mylogit.cv = cv.glmnet(x = dtm_train,
y = train$sentiment,
alpha = 0,
family='binomial',
type.measure = "auc")
mylogit.fit = glmnet(x = dtm_train,
y = train$sentiment,
alpha = 0,
lambda = mylogit.cv$lambda.min,
family='binomial')
#####################################
test <- read.table("test.tsv", stringsAsFactors = FALSE,
header = TRUE)
#####################################
# Compute prediction
# Store your prediction for test data in a data frame
# "output": col 1 is test$id
#           col 2 is the predited probabilities
test$review <- gsub('<.*?>', ' ', test$review)
it_test = itoken(test$review,
preprocessor = tolower,
tokenizer = word_tokenizer)
dtm_test = create_dtm(it_test, vectorizer)
mypred = predict(mylogit.fit, dtm_test, type = "response")
output = data.frame(id = test$id, prob = as.vector(mypred))
#####################################
write.table(output, file = "mysubmission.txt",
row.names = FALSE, sep='\t')
colnames(dtm_train)
max(dtm_train[, "would_love_to"])
?vocab_vectorizer
?create_dtm
head(train)
train$sentiment
?cv.glmnet
knitr::opts_chunk$set(echo = TRUE)
library(e1071)
spam = read.table(file="https://web.stanford.edu/~hastie/ElemStatLearn/datasets/spam.data")
names(spam)[58] = "Y"
spam$Y = as.factor(spam$Y)
testID = c(1:100, 1901:1960)
spam.test=spam[testID, ];
spam.train=spam[-testID, ]
svmfit=svm(Y ~., kernel="linear", data=spam.train, cost=1)
summary(svmfit)
table(spam.train$Y, svmfit$fitted)
svmpred=predict(svmfit, newdata=spam.test)
table(spam.test$Y, svmpred)
svmfit=svm(Y ~., kernel="linear", data=spam.train, cost=10)
summary(svmfit)
table(spam.train$Y, svmfit$fitted)
svmpred=predict(svmfit, newdata=spam.test)
table(spam.test$Y, svmpred)
svmfit=svm(Y ~., kernel="linear", data=spam.train, cost=50)
summary(svmfit)
table(spam.train$Y, svmfit$fitted)
svmpred=predict(svmfit, newdata=spam.test)
table(spam.test$Y, svmpred)
svmfit=svm(Y ~., data=spam.train, cost=1)
summary(svmfit)
table(spam.train$Y, svmfit$fitted)
svmpred=predict(svmfit, newdata=spam.test)
table(spam.test$Y, svmpred)
svmfit=svm(Y ~., data=spam.train, cost=10)
summary(svmfit)
table(spam.train$Y, svmfit$fitted)
svmpred=predict(svmfit, newdata=spam.test)
table(spam.test$Y, svmpred)
svmfit=svm(Y ~., data=spam.train, cost=50)
summary(svmfit)
table(spam.train$Y, svmfit$fitted)
svmpred=predict(svmfit, newdata=spam.test)
table(spam.test$Y, svmpred)
library(lubridate)
library(tidyverse)
library(forecast)
source("mymain.R")
# read in train / test dataframes
train <- readr::read_csv('train_ini.csv')
test <- readr::read_csv('test.csv')
# save weighted mean absolute error WMAE
num_folds <- 10
wae <- rep(0, num_folds)
for (t in 1:num_folds) {
# *** THIS IS YOUR PREDICTION FUNCTION ***
test_pred <- mypredict()
# load fold file
fold_file <- paste0('fold_', t, '.csv')
new_train <- readr::read_csv(fold_file,
col_types = cols())
# extract predictions matching up to the current fold
scoring_tbl <- new_train %>%
left_join(test_pred, by = c('Date', 'Store', 'Dept'))
# compute WMAE
actuals <- scoring_tbl$Weekly_Sales
preds <- scoring_tbl$Weekly_Pred
preds[is.na(preds)] <- 0
weights <- if_else(scoring_tbl$IsHoliday, 5, 1)
wae[t] <- sum(weights * abs(actuals - preds)) / sum(weights)
}
print(wae)
mean(wae)
setwd("C:/Users/User/OneDrive/UIUC/STAT 542 (PSL)/Projects/Project 2/CS-598-Project-2")
library(lubridate)
library(tidyverse)
library(forecast)
source("mymain.R")
# read in train / test dataframes
train <- readr::read_csv('train_ini.csv')
test <- readr::read_csv('test.csv')
# save weighted mean absolute error WMAE
num_folds <- 10
wae <- rep(0, num_folds)
for (t in 1:num_folds) {
# *** THIS IS YOUR PREDICTION FUNCTION ***
test_pred <- mypredict()
# load fold file
fold_file <- paste0('fold_', t, '.csv')
new_train <- readr::read_csv(fold_file,
col_types = cols())
# extract predictions matching up to the current fold
scoring_tbl <- new_train %>%
left_join(test_pred, by = c('Date', 'Store', 'Dept'))
# compute WMAE
actuals <- scoring_tbl$Weekly_Sales
preds <- scoring_tbl$Weekly_Pred
preds[is.na(preds)] <- 0
weights <- if_else(scoring_tbl$IsHoliday, 5, 1)
wae[t] <- sum(weights * abs(actuals - preds)) / sum(weights)
}
print(wae)
mean(wae)
